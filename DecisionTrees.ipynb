{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2146192f-f596-4762-88a3-9bf5792802b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./MLSpe_venv/lib/python3.13/site-packages (2.3.5)\n",
      "Requirement already satisfied: matplotlib in ./MLSpe_venv/lib/python3.13/site-packages (3.10.8)\n",
      "Requirement already satisfied: pandas in ./MLSpe_venv/lib/python3.13/site-packages (2.3.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./MLSpe_venv/lib/python3.13/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./MLSpe_venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./MLSpe_venv/lib/python3.13/site-packages (from matplotlib) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./MLSpe_venv/lib/python3.13/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./MLSpe_venv/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in ./MLSpe_venv/lib/python3.13/site-packages (from matplotlib) (12.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./MLSpe_venv/lib/python3.13/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./MLSpe_venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./MLSpe_venv/lib/python3.13/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./MLSpe_venv/lib/python3.13/site-packages (from pandas) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in ./MLSpe_venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy matplotlib pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01d04161-3f6d-4f7c-b775-5c79dea25144",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e059e6f-e4cd-4185-ae91-8efd3f136499",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([[1,1,1],[1,0,1],[1,0,0],[1,0,0],[1,1,1],[0,1,1],[0,0,0],[1,0,1],[0,1,0],[1,0,0]])\n",
    "y_train = np.array([1,1,0,0,1,0,0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49ce0657-21c2-4445-8052-f7310bd6227e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few elements of X_train:\n",
      " [[1 1 1]\n",
      " [1 0 1]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [1 1 1]]\n",
      "Type of X_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"First few elements of X_train:\\n\", X_train[:5])\n",
    "print(\"Type of X_train:\",type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1086afc-04c6-4d60-97ed-6e029c3aee99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few elements of y_train: [1 1 0 0 1]\n",
      "Type of y_train: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"First few elements of y_train:\", y_train[:5])\n",
    "print(\"Type of y_train:\",type(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc69b3ac-e766-4246-a60c-9434378739ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is: (10, 3)\n",
      "The shape of y_train is:  (10,)\n",
      "Number of training examples (m): 10\n"
     ]
    }
   ],
   "source": [
    "print ('The shape of X_train is:', X_train.shape)\n",
    "print ('The shape of y_train is: ', y_train.shape)\n",
    "print ('Number of training examples (m):', len(X_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a4b011b-adab-4a64-8dd8-04f317a32fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(y):\n",
    "    \"\"\"\n",
    "    Computes the entropy for \n",
    "    \n",
    "    Args:\n",
    "       y (ndarray): Numpy array indicating whether each example at a node is\n",
    "           edible (`1`) or poisonous (`0`)\n",
    "       \n",
    "    Returns:\n",
    "        entropy (float): Entropy at that node\n",
    "        \n",
    "    \"\"\"\n",
    "    # You need to return the following variables correctly\n",
    "    entropy = 0.\n",
    "\n",
    "    if len(y) != 0:\n",
    "        # 1. Calculate the fraction of examples that are class 1 (p1)\n",
    "        # Since y contains only 0s and 1s, the mean is the fraction of 1s.\n",
    "        p1 = len(y[y == 1]) / len(y)\n",
    "        \n",
    "        # 2. Handle the edge case where the node is pure (p1 is 0 or 1)\n",
    "        # If p1 is 0 or 1, entropy is 0. If we try to calculate log2(0), we get an error/NaN.\n",
    "        if p1 != 0 and p1 != 1:\n",
    "            # 3. Apply the entropy formula\n",
    "            entropy = -p1 * np.log2(p1) - (1 - p1) * np.log2(1 - p1)\n",
    "        else:\n",
    "            entropy = 0.\n",
    "       \n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89f418cc-7d4c-4ea5-9962-cea2ff2a1e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy at root node:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Entropy at root node: \", compute_entropy(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a90f036-652f-4c70-b603-c20be58abe12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, node_indices, feature):\n",
    "    \"\"\"\n",
    "    Splits the data at the given node into\n",
    "    left and right branches\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):             Data matrix of shape(n_samples, n_features)\n",
    "        node_indices (list):     List containing the active indices. I.e, the samples being considered at this step.\n",
    "        feature (int):           Index of feature to split on\n",
    "    \n",
    "    Returns:\n",
    "        left_indices (list):     Indices with feature value == 1\n",
    "        right_indices (list):    Indices with feature value == 0\n",
    "    \"\"\"\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    left_indices = []\n",
    "    right_indices = []\n",
    "    \n",
    "\n",
    "    # Go through every index present at this node\n",
    "    for i in node_indices:\n",
    "        \n",
    "        # Check the value of the specific feature for this data point\n",
    "        if X[i, feature] == 1:\n",
    "            left_indices.append(i)\n",
    "        else:\n",
    "            right_indices.append(i)\n",
    "        \n",
    "    return left_indices, right_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f21ac877-76bf-4f62-a8d8-5813a936b7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CASE 1:\n",
      "Left indices:  [0, 1, 2, 3, 4, 7, 9]\n",
      "Right indices:  [5, 6, 8]\n",
      "CASE 2:\n",
      "Left indices:  [0, 2, 4]\n",
      "Right indices:  [6, 8]\n"
     ]
    }
   ],
   "source": [
    "# Case 1\n",
    "\n",
    "root_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "\n",
    "# Feel free to play around with these variables\n",
    "# The dataset only has three features, so this value can be 0 (Brown Cap), 1 (Tapering Stalk Shape) or 2 (Solitary)\n",
    "feature = 0\n",
    "\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices, feature)\n",
    "\n",
    "print(\"CASE 1:\")\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n",
    "\n",
    "\n",
    "\n",
    "# Case 2\n",
    "\n",
    "root_indices_subset = [0, 2, 4, 6, 8]\n",
    "left_indices, right_indices = split_dataset(X_train, root_indices_subset, feature)\n",
    "\n",
    "print(\"CASE 2:\")\n",
    "print(\"Left indices: \", left_indices)\n",
    "print(\"Right indices: \", right_indices)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfaf2c39-d87f-4e6d-bd14-a4beca6ddbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_information_gain(X, y, node_indices, feature):\n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the information of splitting the node on a given feature\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        feature (int):          Index of feature to split on\n",
    "   \n",
    "    Returns:\n",
    "        cost (float):        Cost computed\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Split dataset\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, feature)\n",
    "    \n",
    "    # Some useful variables\n",
    "    X_node, y_node = X[node_indices], y[node_indices]\n",
    "    X_left, y_left = X[left_indices], y[left_indices]\n",
    "    X_right, y_right = X[right_indices], y[right_indices]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    information_gain = 0\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 1. Compute entropy of the current node (parent)\n",
    "    node_entropy = compute_entropy(y_node)\n",
    "    \n",
    "    # 2. Compute entropy of the left and right branches (children)\n",
    "    left_entropy = compute_entropy(y_left)\n",
    "    right_entropy = compute_entropy(y_right)\n",
    "    \n",
    "    # 3. Calculate the weights (fraction of samples in each branch)\n",
    "    w_left = len(X_left) / len(X_node)\n",
    "    w_right = len(X_right) / len(X_node)\n",
    "    \n",
    "    # 4. Calculate the weighted entropy\n",
    "    weighted_entropy = (w_left * left_entropy) + (w_right * right_entropy)\n",
    "    \n",
    "    # 5. Calculate Information Gain\n",
    "    information_gain = node_entropy - weighted_entropy\n",
    "    \n",
    "\n",
    "    \n",
    "    return information_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55ae24bb-f0e6-4fd5-8c3a-43180bf4aff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain from splitting the root on brown cap:  0.034851554559677034\n",
      "Information Gain from splitting the root on tapering stalk shape:  0.12451124978365313\n",
      "Information Gain from splitting the root on solitary:  0.2780719051126377\n"
     ]
    }
   ],
   "source": [
    "info_gain0 = compute_information_gain(X_train, y_train, root_indices, feature=0)\n",
    "print(\"Information Gain from splitting the root on brown cap: \", info_gain0)\n",
    "\n",
    "info_gain1 = compute_information_gain(X_train, y_train, root_indices, feature=1)\n",
    "print(\"Information Gain from splitting the root on tapering stalk shape: \", info_gain1)\n",
    "\n",
    "info_gain2 = compute_information_gain(X_train, y_train, root_indices, feature=2)\n",
    "print(\"Information Gain from splitting the root on solitary: \", info_gain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fca5411b-c39f-482a-a0eb-170c7b440340",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_split(X, y, node_indices):   \n",
    "    \"\"\"\n",
    "    Returns the optimal feature and threshold value\n",
    "    to split the node data \n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "\n",
    "    Returns:\n",
    "        best_feature (int):     The index of the best feature to split\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Some useful variables\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    # You need to return the following variables correctly\n",
    "    best_feature = -1\n",
    "    \n",
    "    \n",
    "    max_info_gain = 0\n",
    "    \n",
    "    # Iterate through all features\n",
    "    for feature in range(num_features):\n",
    "        \n",
    "        # Compute the information gain for the current feature\n",
    "        info_gain = compute_information_gain(X, y, node_indices, feature)\n",
    "        \n",
    "        # If the information gain is better than the current max, update the max and best_feature\n",
    "        if info_gain > max_info_gain:\n",
    "            max_info_gain = info_gain\n",
    "            best_feature = feature\n",
    "            \n",
    "        \n",
    "\n",
    "   \n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb1d5bc5-a6f0-4703-aa00-37c1e2f21a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best feature to split on: 2\n"
     ]
    }
   ],
   "source": [
    "best_feature = get_best_split(X_train, y_train, root_indices)\n",
    "print(\"Best feature to split on: %d\" % best_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7222d14a-ce10-431f-ad2f-a0db49dde792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree_recursive(X, y, node_indices, branch_name, max_depth, current_depth):\n",
    "    \"\"\"\n",
    "    Build a tree using the recursive algorithm that split the dataset into 2 subgroups at each node.\n",
    "    This function just prints the tree.\n",
    "    \n",
    "    Args:\n",
    "        X (ndarray):            Data matrix of shape(n_samples, n_features)\n",
    "        y (array like):         list or ndarray with n_samples containing the target variable\n",
    "        node_indices (ndarray): List containing the active indices. I.e, the samples being considered in this step.\n",
    "        branch_name (string):   Name of the branch. ['Root', 'Left', 'Right']\n",
    "        max_depth (int):        Max depth of the resulting tree. \n",
    "        current_depth (int):    Current depth. Parameter used during recursive call.\n",
    "   \n",
    "    \"\"\" \n",
    "\n",
    "    # Maximum depth reached - stop splitting\n",
    "    if current_depth == max_depth:\n",
    "        formatting = \" \"*current_depth + \"-\"*current_depth\n",
    "        print(formatting, \"%s leaf node with indices\" % branch_name, node_indices)\n",
    "        return\n",
    "   \n",
    "    # Otherwise, get best split and split the data\n",
    "    # Get the best feature and threshold at this node\n",
    "    best_feature = get_best_split(X, y, node_indices) \n",
    "    \n",
    "    formatting = \"-\"*current_depth\n",
    "    print(\"%s Depth %d, %s: Split on feature: %d\" % (formatting, current_depth, branch_name, best_feature))\n",
    "    \n",
    "    # Split the dataset at the best feature\n",
    "    left_indices, right_indices = split_dataset(X, node_indices, best_feature)\n",
    "    tree.append((left_indices, right_indices, best_feature))\n",
    "    \n",
    "    # continue splitting the left and the right child. Increment current depth\n",
    "    build_tree_recursive(X, y, left_indices, \"Left\", max_depth, current_depth+1)\n",
    "    build_tree_recursive(X, y, right_indices, \"Right\", max_depth, current_depth+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a59a0387-a80a-4932-baaf-973d0d7ff3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Depth 0, Root: Split on feature: 2\n",
      "- Depth 1, Left: Split on feature: 0\n",
      "  -- Left leaf node with indices [0, 1, 4, 7]\n",
      "  -- Right leaf node with indices [5]\n",
      "- Depth 1, Right: Split on feature: 1\n",
      "  -- Left leaf node with indices [8]\n",
      "  -- Right leaf node with indices [2, 3, 6, 9]\n"
     ]
    }
   ],
   "source": [
    "tree = []\n",
    "build_tree_recursive(X_train, y_train, root_indices, \"Root\", max_depth=2, current_depth=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4501bef-af22-4c4b-9be2-f5faf577439e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
